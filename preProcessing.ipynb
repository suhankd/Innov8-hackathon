{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Language detection and translation libraries.\n",
    "\n",
    "from deep_translator import GoogleTranslator # Google translate API accessor.\n",
    "\n",
    "# Language Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize # Tokenizing the message\n",
    "from nltk.stem import WordNetLemmatizer # Lemmatization of the message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "\n",
    "1. Aniket pointed out that a lot of the words are in latin. `deep_translator` is a library for language translation. `GoogleTranslator` is an object that uses Google Translate's API for translation. We want to convert the latin words into english.\n",
    "\n",
    "2. A lot of the words used are in english, except with an 'x' or a 'z' at the ending. Removing this suffix would make the word closer to english, while also removing a garbage character that doesn't add any context/meaning to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>fingers</th>\n",
       "      <th>tail</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pluvia arbor aquos</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>Aquari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cosmix xeno nebuz odbitaz</td>\n",
       "      <td>5</td>\n",
       "      <td>yes</td>\n",
       "      <td>Zorblax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>solarix glixx novum galaxum quasar</td>\n",
       "      <td>5</td>\n",
       "      <td>yes</td>\n",
       "      <td>Zorblax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arbor insectus pesros ekos dootix nimbus</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>Florian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mermax drakos lorix epikoz deftax</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>Faerix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>empathix sadix disgux dredax pridius afgstix e...</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>Emotivor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>quasar ustron nebulax meteorn</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>Quixnar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>astron xeno ceaestar astron kometa</td>\n",
       "      <td>6</td>\n",
       "      <td>yes</td>\n",
       "      <td>Zorblax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>sporzom nimbus terram terranix aviana ekos nimbub</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>Florian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>blastix titanos relikum drakos gryphox sirenix</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>Mythron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               message  fingers tail   species\n",
       "0                                   pluvia arbor aquos        4   no    Aquari\n",
       "1                            cosmix xeno nebuz odbitaz        5  yes   Zorblax\n",
       "2                   solarix glixx novum galaxum quasar        5  yes   Zorblax\n",
       "3             arbor insectus pesros ekos dootix nimbus        2  yes   Florian\n",
       "4                    mermax drakos lorix epikoz deftax        4   no    Faerix\n",
       "..                                                 ...      ...  ...       ...\n",
       "495  empathix sadix disgux dredax pridius afgstix e...        2   no  Emotivor\n",
       "496                      quasar ustron nebulax meteorn        4   no   Quixnar\n",
       "497                 astron xeno ceaestar astron kometa        6  yes   Zorblax\n",
       "498  sporzom nimbus terram terranix aviana ekos nimbub        2  yes   Florian\n",
       "499     blastix titanos relikum drakos gryphox sirenix        4  yes   Mythron\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes 'x' and 'z' from the ending.\n",
    "\n",
    "def cleanEnding(message):\n",
    "\n",
    "    cleaned_message = []\n",
    "    message = message.split()\n",
    "    \n",
    "    for word in message:\n",
    "        if word[-1] in ['x','z']:\n",
    "            word = word[:-1]\n",
    "        cleaned_message.append(word)\n",
    "    return ' '.join(cleaned_message)\n",
    "\n",
    "# Convert latin words to english.\n",
    "\n",
    "def englishConversion(message):\n",
    "\n",
    "    translator = GoogleTranslator(source=\"latin\",target=\"english\") # Initialize the translator.\n",
    "\n",
    "    translatedMessage = []\n",
    "    message = message.split(' ')\n",
    "\n",
    "    for word in message:\n",
    "        translatedWord = translator.translate(word)\n",
    "        translatedMessage.append(translatedWord)\n",
    "    \n",
    "    return ' '.join(translatedMessage)\n",
    "\n",
    "def processing(message):\n",
    "\n",
    "    message = englishConversion(cleanEnding(message))\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer() # Lemmatizing, now that it is similar to english.\n",
    "    tokens = word_tokenize(message) # Tokenization, so the model can understand better.\n",
    "\n",
    "    lemmatizations = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_message'] = df['message'].apply(processing) # Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the data further\n",
    "\n",
    "def join(message):\n",
    "    return ' '.join(word for word in message)\n",
    "\n",
    "from nltk.corpus import stopwords # Stopwords, i.e. garbage words that provide no extra context/meaning to the model.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "additional_stopwords = ['i','a'] # Words I found in the data that are useless. You guys can add more if you want.\n",
    "stopwords_list.extend(additional_stopwords)\n",
    "\n",
    "def remove_stopwords(message):\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for word in message:\n",
    "        if word not in stopwords_list:\n",
    "            res.append(word.lower())\n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "df['processedMessage'] = df['preprocessed_message'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to handle the multimodal distribution of the number of fingers of the \"Faerix\" and \"Emotivor\" species. Through observing the KDE graph, we can see that the slope is 0 for Faerix when x = 3.5, and for Emotivor when x = 1.5. So let's separate the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Aquari', 'Zorblax', 'Florian', 'Faerix_Group2', 'Nexoon',\n",
       "       'Mythron', 'Emotivor_Group2', 'Sentire', 'Quixnar', 'Cybex',\n",
       "       'Emotivor_Group1', 'Faerix_Group1'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_species(row):\n",
    "    if row[\"species\"] == \"Faerix\":\n",
    "        if row[\"fingers\"] <= 3.5:\n",
    "            return \"Faerix_Group1\"\n",
    "        else:\n",
    "            return \"Faerix_Group2\"\n",
    "    elif row[\"species\"] == \"Emotivor\":\n",
    "        if row[\"fingers\"] <= 1.5:\n",
    "            return \"Emotivor_Group1\"\n",
    "        else:\n",
    "            return \"Emotivor_Group2\"\n",
    "    return row[\"species\"]\n",
    "\n",
    "df[\"species_group\"] = df.apply(split_species, axis=1)\n",
    "\n",
    "df['species_group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"processedData.csv\") # Storing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts :\n",
    "\n",
    "1. I am running the entire message through the Latin-English translator. If the translator sees any latin words, it converts it, and leaves anything it doesn't recognize. However this isn't always true, as sometimes it might make an error and translate a non-latin word, which usually returns the same word but fucked up by a little. Example : \"pollex\" -> \"pull\". All the language detection libraries I've seen aren't very reliable, so this is something we need to make allowances for. Think up ideas to get past this, or language detection frameworks we can use. Build a latin/Non latin classification model might work.\n",
    "\n",
    "2. I added 'i' and 'a' to the stopwords list by looking through the processed data and seeing what stood out. If you have time, do the same, it would help a lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
