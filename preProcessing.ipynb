{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Language detection and translation libraries.\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Language Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize # Tokenizing the message\n",
    "from nltk.stem import WordNetLemmatizer # Lemmatization of the message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "\n",
    "1. Aniket pointed out that a lot of the words are in latin. `deep_translator` is a library for language translation. `GoogleTranslator` is an object that uses Google Translate's API for translation. We want to convert the latin words into english.\n",
    "\n",
    "2. A lot of the words used are in english, except with an 'x' or a 'z' at the ending. Removing this suffix would make the word closer to english, while also removing a garbage character that doesn't add any context/meaning to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>fingers</th>\n",
       "      <th>tail</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pluvia arbor aquos</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>Aquari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cosmix xeno nebuz odbitaz</td>\n",
       "      <td>5</td>\n",
       "      <td>yes</td>\n",
       "      <td>Zorblax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>solarix glixx novum galaxum quasar</td>\n",
       "      <td>5</td>\n",
       "      <td>yes</td>\n",
       "      <td>Zorblax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arbor insectus pesros ekos dootix nimbus</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>Florian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mermax drakos lorix epikoz deftax</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>Faerix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>empathix sadix disgux dredax pridius afgstix e...</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>Emotivor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>quasar ustron nebulax meteorn</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "      <td>Quixnar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>astron xeno ceaestar astron kometa</td>\n",
       "      <td>6</td>\n",
       "      <td>yes</td>\n",
       "      <td>Zorblax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>sporzom nimbus terram terranix aviana ekos nimbub</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>Florian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>blastix titanos relikum drakos gryphox sirenix</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>Mythron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               message  fingers tail   species\n",
       "0                                   pluvia arbor aquos        4   no    Aquari\n",
       "1                            cosmix xeno nebuz odbitaz        5  yes   Zorblax\n",
       "2                   solarix glixx novum galaxum quasar        5  yes   Zorblax\n",
       "3             arbor insectus pesros ekos dootix nimbus        2  yes   Florian\n",
       "4                    mermax drakos lorix epikoz deftax        4   no    Faerix\n",
       "..                                                 ...      ...  ...       ...\n",
       "495  empathix sadix disgux dredax pridius afgstix e...        2   no  Emotivor\n",
       "496                      quasar ustron nebulax meteorn        4   no   Quixnar\n",
       "497                 astron xeno ceaestar astron kometa        6  yes   Zorblax\n",
       "498  sporzom nimbus terram terranix aviana ekos nimbub        2  yes   Florian\n",
       "499     blastix titanos relikum drakos gryphox sirenix        4  yes   Mythron\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes 'x' and 'z' from the ending.\n",
    "\n",
    "def cleanEnding(message):\n",
    "\n",
    "    cleaned_message = []\n",
    "    message = message.split()\n",
    "    \n",
    "    for word in message:\n",
    "        if word[-1] in ['x','z']:\n",
    "            word = word[:-1]\n",
    "        cleaned_message.append(word)\n",
    "    return ' '.join(cleaned_message)\n",
    "\n",
    "# Convert latin words to english.\n",
    "\n",
    "def englishConversion(message):\n",
    "\n",
    "    translator = GoogleTranslator(source=\"latin\",target=\"english\") # Initialize the translator.\n",
    "\n",
    "    translatedMessage = []\n",
    "    message = message.split(' ')\n",
    "\n",
    "    for word in message:\n",
    "        translatedWord = translator.translate(word)\n",
    "        translatedMessage.append(translatedWord)\n",
    "    \n",
    "    return ' '.join(translatedMessage)\n",
    "\n",
    "def processing(message):\n",
    "\n",
    "    message = englishConversion(cleanEnding(message))\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(message)\n",
    "\n",
    "    lemmatizations = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatizations\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_message'] = df['message'].apply(processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suhan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the data further\n",
    "\n",
    "def join(message):\n",
    "    return ' '.join(word for word in message)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(message):\n",
    "\n",
    "    res = []\n",
    "    message = message.split()\n",
    "\n",
    "    for word in message:\n",
    "        if word not in stopwords_list:\n",
    "            res.append(word)\n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "df['preprocessed_message_str'] = df['preprocessed_message'].apply(join)\n",
    "df['preprocessed_message_str'] = df['preprocessed_message_str'].apply(remove_stopwords)\n",
    "\n",
    "df.rename(columns={'preprocessed_message_str':'processedMessage'},inplace=True)\n",
    "df.to_csv(\"processedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
